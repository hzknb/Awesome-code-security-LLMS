# Awesome-code-security-LLMS

Awesome-code-security-LLMs is a curated list of resources, tools, and research papers related to code security and large language models (LLMs). The goal is to provide a comprehensive collection of materials that can help researchers, developers, and practitioners in the field of code security and LLMS.

## Table of Contents

| Time    | Title             |Venue            |Description           |Paper                   |Resources                   |
|:-------:|------------------|:---------------:|---------------------|:----------------------:|:----------------------:|
| 2021.07 | **CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software** | PROMISE'21 |CVEfixes automates the process of collecting, classifying, and analyzing vulnerability fixes from open-source repositories, providing a rich dataset to advance research in software security.|[link](https://arxiv.org/abs/2107.08760)|[code](https://github.com/secureIT-project/CVEfixes)<br>[dataset](https://zenodo.org/records/13118970)|
| 2023.02 | **CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models** | SaTML'24 |This paper evaluates security vulnerabilities in black-box code generation models using few-shot prompting and CodeQL analysis.|[link](https://arxiv.org/abs/2302.04012)|-|
| 2024.06 | **Can We Trust Large Language Models Generated Code? A Framework for In-Context Learning, Security Patterns, and Code Evaluations Across Diverse LLMs** | arxiv |This paper introduces an ICL-based framework to enhance LLM-generated code security and evaluates risks using static analysis and CSRM.|[link](https://arxiv.org/abs/2406.12513)||
| 2025.01 | **CWEVAL: Outcome-driven Evaluation on Functionality and Security of LLM Code Generation** | LLM4Code'25 |This paper introduces CWEVAL, a comprehensive evaluation framework that assesses both the functionality and security of LLM-generated code using CWEVAL-BENCH, revealing critical gaps in traditional benchmarks and highlighting potential alignment tax in security-focused fine-tuning.|[link](https://arxiv.org/abs/2501.08200)|[code](https://github.com/Co1lin/CWEval)<br>[benchmark](https://arise-lab.github.io/cweval-bench/)|
| 2025.01 | **Large Language Models and Code Security: A Systematic Literature Review** | arxiv |This paper reviews the security implications of using LLMs for code tasks, focusing on their ability to introduce, detect, and fix vulnerabilities, and analyzes the effects of prompting strategies and data poisoning.|[link](https://arxiv.org/abs/2412.15004)|-|
| 2025.02 | **Security Weaknesses of Copilot-Generated Code in GitHub Projects: An Empirical Study** | TOSEM'25 |This study analyzes the security of AI-generated code from GitHub Copilot and similar tools and evaluates Copilot Chat's ability to fix identified vulnerabilities.|[link](https://arxiv.org/abs/2310.02059)|[dataset](https://zenodo.org/records/14565942)|

